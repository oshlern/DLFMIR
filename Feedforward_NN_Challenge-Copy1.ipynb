{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: MIR with Feedforward Neural Networks #\n",
    "\n",
    "You will be assigned to a team of ~4 students. The goal is very straight forward:\n",
    "\n",
    "* Design the feedforwad Neural Network that achieves the best \"test\" performance on the Iowa Musical Instrument Samples and the GTZAN datasets.\n",
    "\n",
    "* We suggest that you use python and tensorflow, but if you have more experience using another toolbox or language, feel free to use anything.\n",
    "\n",
    "* Divide work among team members: \n",
    "    * Individuals train different models in parallel on their computers \n",
    "    * Continuously share results and progress with the team \n",
    "    * Always help each other\n",
    "    \n",
    "\n",
    "* In an email to the members of the teaching staff, submit your results as a jupyter notebook (or similar script) that clearly shows the architecture of the model, your training and cross-validation of hyperparameters, and the test accuracy.\n",
    "\n",
    "* NEVER use the test-set until you have finished tuning your hyper-parameters.\n",
    "\n",
    "* Although the model MUST BE a simple feedforward Neural Network, you may research about and use ways to improve these simple models. Some ideas include: dropout, regularization, momentum, and data augmentation, but there are many more. Always include clear comments in your code that indicate what you are doing, with links and references to original papers/websites/ideas.\n",
    "\n",
    "* The team achieving the best result will be pinned on r/DLfMIR.\n",
    "\n",
    "* You may continue submitting your individual results even after the workshop is over. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random, time, copy\n",
    "# from utils import show_graph\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's reload the music dataset we used previously\n",
    "dataset = scipy.io.loadmat('../data/GTZAN_small.mat')\n",
    "data = dataset['dat_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general data parameters\n",
    "N = data.shape[0]\n",
    "D = data.shape[1]-1\n",
    "C = 10\n",
    "\n",
    "# split into training, validation, and test sets\n",
    "perc_tr = 0.8\n",
    "perc_vl = 0.1\n",
    "perc_ts = 0.1\n",
    "\n",
    "# randomly shuffle the data (just to make sure)\n",
    "np.random.permutation(data)\n",
    "\n",
    "# separate into training, validation, and test sets\n",
    "# data\n",
    "x_tr = data[:int(N*perc_tr),0:-1]\n",
    "x_vl = data[int(N*perc_tr):int(N*perc_tr+N*perc_vl),0:-1]\n",
    "x_ts = data[-int(N*perc_ts):,0:-1]\n",
    "# labels\n",
    "y_tr = data[:int(N*perc_tr),-1].reshape(int(N*perc_tr),1)\n",
    "y_vl = data[int(N*perc_tr):int(N*perc_tr+N*perc_vl),-1].reshape(int(N*perc_vl),1)\n",
    "y_ts = data[-int(N*perc_ts):,-1].reshape(int(N*perc_ts),1)\n",
    "\n",
    "# we won't use the variable `data` after this point.\n",
    "del data\n",
    "\n",
    "temp = np.zeros((y_tr.shape[0],C))\n",
    "temp[np.arange(y_tr.shape[0]),y_tr.astype(int)[:,0]] = 1\n",
    "y_tr = temp\n",
    "temp = np.zeros((y_vl.shape[0],C))\n",
    "temp[np.arange(y_vl.shape[0]),y_vl.astype(int)[:,0]] = 1\n",
    "y_vl = temp\n",
    "temp = np.zeros((y_ts.shape[0],C))\n",
    "temp[np.arange(y_ts.shape[0]),y_ts.astype(int)[:,0]] = 1\n",
    "y_ts = temp\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jacobian(y, x): # NO LOG\n",
    "    shapey = list(y.shape)\n",
    "    print type(shapey), shapey\n",
    "    if len(shapey) == 1:\n",
    "        return tf.convert_to_tensor([tf.gradients(y[i],x) for i in range(int(shapey[0]))])\n",
    "    elif len(shapey) == 2:\n",
    "        return tf.convert_to_tensor([[tf.gradients(y[i,j],x) for j in range(int(shapey[1]))] for i in range(int(shapey[0]))])\n",
    "    elif len(shapey) == 3:\n",
    "        return tf.convert_to_tensor([[[tf.gradients(y[i,j,k],x) for k in range(int(shapey[2]))] for j in range(int(shapey[1]))] for i in range(int(shapey[0]))])\n",
    "    else:\n",
    "        return tf.gradients(y,x)\n",
    "\n",
    "def get_batch(size=20):\n",
    "    inds = np.random.choice(N, size, replace=False)\n",
    "    x, y = x_tr[inds,:], y_tr[inds,:]\n",
    "    return x, y\n",
    "\n",
    "def train(sess, nepochs=25, batch_size=25, calc_loss=True):\n",
    "    if calc_loss:\n",
    "        losses = np.ndarray(shape=(nepochs), dtype=float)\n",
    "    for i in range(nepochs):\n",
    "#         x, y = x_tr, y_tr\n",
    "#         x, y = get_batch(size=batch_size)\n",
    "#         x, y = tf.train.batch([x_tr, y_tr], batch_size)\n",
    "        sess.run(GD_step, feed_dict={X: x_tr, y: y_tr})\n",
    "        if calc_loss:\n",
    "            loss = sess.run(loss_f, feed_dict={X: x_tr, y: y_tr})\n",
    "            losses[i] = loss\n",
    "            print \"epoch \", i, \" loss \", loss\n",
    "    if calc_loss:\n",
    "        return losses\n",
    "\n",
    "def test():\n",
    "    predicted_labels = sess.run(max_y_index, feed_dict={X: x_vl, y: y_vl})\n",
    "    vl_acc = np.mean(predicted_labels == np.argmax(y_vl, axis = 1))\n",
    "    print \"The accuracy on the validation set is: \", vl_acc\n",
    "    predicted_labels = sess.run(max_y_index, feed_dict={X: x_tr, y: y_tr})\n",
    "    tr_acc = np.mean(predicted_labels == np.argmax(y_tr, axis = 1))\n",
    "    print \"The accuracy on the training set is: \", tr_acc\n",
    "    return vl_acc\n",
    "\n",
    "def graph(losses):\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "    time.sleep(5)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 44101 10\n"
     ]
    }
   ],
   "source": [
    "# general parameters\n",
    "N = x_tr.shape[0] # number of training examples\n",
    "D = x_tr.shape[1] # dimensionality of the data\n",
    "C = y_tr.shape[1] # number of unique labels in the dataset\n",
    "print N, D, C\n",
    "# hyperparameters\n",
    "H = 128 # number of hidden units. In general try to stick to a power of 2\n",
    "lr = 0.001 # the learning rate (previously refered to in the notes as alpha)\n",
    "dr = 0.4\n",
    "stand_dev = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's initialize the weights\n",
    "W_h = tf.Variable(tf.random_normal((D,H), stddev = 0.01))\n",
    "W_o = tf.Variable(tf.random_normal((H,C), stddev = 0.01))\n",
    "B_h = tf.Variable(tf.random_normal((1,H), stddev = 0.01))\n",
    "B_o = tf.Variable(tf.random_normal((1,C), stddev = 0.01))\n",
    "\n",
    "Ws = [W_h, B_h, W_o, B_o]\n",
    "\n",
    "X = tf.placeholder(\"float\", shape=[None,D])\n",
    "y = tf.placeholder(\"float\", shape=[None,C])\n",
    "\n",
    "# we now do the forward pass until we obtain the scores\n",
    "h1 = tf.layers.dense(X,100,tf.nn.softmax)\n",
    "# h2 = tf.layers.dense(tf.layers.dropout(h1,rate=dr),100,tf.nn.softmax)\n",
    "# h3 = tf.layers.dense(tf.layers.dropout(h2,rate=dr),50,tf.nn.relu)\n",
    "# h4 = tf.layers.dense(tf.layers.dropout(h1,rate=dr),20,tf.nn.relu)\n",
    "# scores = tf.layers.dense(h4,C,tf.nn.softmax)\n",
    "h = tf.nn.sigmoid(tf.add(tf.matmul(X,W_h), B_h))\n",
    "# h2 = tf.nn.relu(tf.add(tf.matmul(X,W_h), B_h))\n",
    "# h3 = tf.nn.relu(tf.add(tf.matmul(X,W_h), B_h))\n",
    "# h4 = tf.nn.relu(tf.add(tf.matmul(X,W_h), B_h))\n",
    "scores = tf.add(tf.matmul(h, W_o), B_o)\n",
    "y_hat = tf.nn.softmax(scores)\n",
    "max_y_index = tf.argmax(y_hat, axis = 1)\n",
    "# sampled_y_index = tf.to_int32(tf.multinomial(tf.log(y_hat), 1)[0][0])\n",
    "# print y_hat.shape\n",
    "loss_f = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=scores))\n",
    "# GD_step = tf.train.GradientDescentOptimizer(lr).minimize(loss_f)\n",
    "GD_step = tf.train.AdamOptimizer(lr).minimize(loss_f)\n",
    "\n",
    "# FISHER INFOS\n",
    "# jacobian = tf.gradients(tf.log(y_hat[0,sampled_y_index]), Ws)\n",
    "# ys = copy.deepcopy(y_hat)\n",
    "# print y_hat\n",
    "# jacobians = tf.gradients(tf.log(y_hat), Ws)#, grad_ys=y_hat) # of shape Ws. Shouldn't it depend on shape of y and form a tensor?\n",
    "# print y_hat\n",
    "# jacobians = get_jacobian(y_hat, X)\n",
    "\n",
    "\n",
    "# print len(jacobians)\n",
    "# for grad in jacobians:\n",
    "#     print grad.shape\n",
    "# lin_fisher = [tf.multiply(stand_dev, tf.matmul(tf.transpose(jacobian), jacobian)) for jacobian in jacobians]\n",
    "\n",
    "# # fishy_softmax = tf.diag(tf.divide(1,y)) # fishy because its a helper to fisher using y, the output\n",
    "# # fishy doesn't depend on activation function of W or W. but the dimensions should match\n",
    "# # fisher should be of dimension W.shape[0]xW.shape[0] so that it's inverse can be multiplied by the grad of J wrt W (dJ_dW) of shape W\n",
    "# # datapoint by datapoint or batches? (expected value, divide later)\n",
    "# fishy_softmax = tf.divide(1,y) # fishy because its a helper to fisher using y, the output\n",
    "# print fishy_softmax.shape\n",
    "# print jacobians[2].shape\n",
    "# print \"CHECK\"\n",
    "# print y[2].shape\n",
    "# print tf.divide(jacobians[2],y[2][None,:])\n",
    "# def fisher(jacobian, W, out):\n",
    "#         fishy = tf.divide(1,y)\n",
    "#         for jacobian in jacobians:\n",
    "#             total = np.zeros(jacobian.shape)\n",
    "#             for yi in y:\n",
    "#                 total += tf.matmul(tf.transpose(jacobian), tf.divide(jacobian,tf.divide(1,y)))\n",
    "            \n",
    "#         [sum(tf.matmul(tf.transpose(jacobian), tf.divide(jacobian,tf.divide(1,y)))) for jacobian in jacobians]\n",
    "# softmax_fisher = [sum(tf.matmul(tf.transpose(jacobian), tf.divide(jacobian,tf.divide(1,y))) for jacobian in jacobians]\n",
    "\n",
    "# fishy_sigmoid = tf.diag(tf.divide(tf.divide(1,y),tf.subtract(1,y))) # fishy because its a helper to fisher using y, the output\n",
    "# fishy_sigmoid = tf.divide(tf.divide(1,y),tf.subtract(1,y)) # fishy because its a helper to fisher using y, the output\n",
    "# sigmoid_fisher = [tf.matmul(tf.transpose(jacobian), tf.matmul(fishy_sigmoid, jacobian)) for jacobian in jacobians]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "finished init\n",
      "1\n",
      "<type 'list'> [Dimension(10)]\n",
      "(10, 1, 128, 10)\n"
     ]
    }
   ],
   "source": [
    "# print x_tr[1:100,:].shape, D\n",
    "# print y_tr[1:100,:].shape, C\n",
    "# print x_tr.shape\n",
    "print \"init\"\n",
    "# print x_tr[0:1,:].shape\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print \"finished init\"\n",
    "# print y_hat\n",
    "# grads = sess.run(tf.gradients(tf.log(y_hat), Ws), feed_dict={X: x_tr, y: y_tr})#, grad_ys=y_hat)) # of shape Ws. Shouldn't it depend on shape of y and form a tensor?\n",
    "# y_hat = sess.run(y_hat, feed_dict={X: x_tr, y: y_tr})\n",
    "# y_hat = sess.run(y_hat, feed_dict={X: x_tr[1:2,:], y: y_tr[1:2,:]})\n",
    "print len(sess.run(tf.gradients(y_hat[0], W_o), feed_dict={X: x_tr[0:1,:], y: y_tr[0:1,:]}))\n",
    "# print y_hat.shape\n",
    "jacobians = sess.run(get_jacobian(y_hat[0,:], W_o), feed_dict={X: x_tr[0:1,:], y: y_tr[0:1,:]})\n",
    "print jacobians.shape\n",
    "# with sess.as_default():\n",
    "    \n",
    "\n",
    "    #     print jacobians.eval(feed_dict={X: x_tr, y: y_tr})\n",
    "# print sess.run(loss_f, feed_dict={X: x_tr, y: y_tr})\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print \"hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44101,)\n",
      "(1, 44101)\n",
      "(2047, 44101)\n",
      "(1, 1, 97, 44101)\n",
      "4\n",
      "(44101, 128)\n",
      "(1, 128)\n",
      "(128, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print x_tr[np.random.randint(N)].shape\n",
    "print np.array([x_tr[np.random.randint(N)]]).shape\n",
    "\n",
    "print x_tr[0:np.random.randint(N),:].shape\n",
    "print np.array([[x_tr[0:np.random.randint(N),:]]]).shape\n",
    "grads = sess.run(jacobians, feed_dict={X: np.array([x_tr[np.random.randint(N)]])})\n",
    "# for F_w in F:\n",
    "#     F_w = \n",
    "# print grads.get_shape()\n",
    "# print grads.shape()\n",
    "print len(grads)\n",
    "for grad in grads:\n",
    "    print grad.shape\n",
    "sess.close()\n",
    "#  def compute_fisher(data, sess, plot_diffs=False, disp_freq=10):\n",
    "#         # computer Fisher information for each parameter\n",
    "\n",
    "#         # initialize Fisher information for most recent task\n",
    "#         F = []\n",
    "#         for weights in Ws:\n",
    "#             F.append(np.zeros(weights.get_shape().as_list())\n",
    "\n",
    "#         # sampling a random class from softmax\n",
    "        \n",
    "\n",
    "#         if(plot_diffs):\n",
    "#             # track differences in mean Fisher info\n",
    "#             F_prev = deepcopy(F)\n",
    "#             mean_diffs = np.zeros(0)\n",
    "\n",
    "#         for i in range(num_samples):\n",
    "#             # compute first-order derivatives\n",
    "#             grads = sess.run(jacobian, feed_dict={X: x_tr[np.random.randint(N)]})\n",
    "#             # square the derivatives and add to total\n",
    "#             for i in range(len(F)):\n",
    "#                 F[i] += np.square(ders[i])\n",
    "#             if(plot_diffs):\n",
    "#                 if i % disp_freq == 0 and i > 0:\n",
    "#                     # recording mean diffs of F\n",
    "#                     F_diff = 0\n",
    "#                     for v in range(len(self.F_accum)):\n",
    "#                         F_diff += np.sum(np.absolute(self.F_accum[v]/(i+1) - F_prev[v]))\n",
    "#                     mean_diff = np.mean(F_diff)\n",
    "#                     mean_diffs = np.append(mean_diffs, mean_diff)\n",
    "#                     for v in range(len(self.F_accum)):\n",
    "#                         F_prev[v] = self.F_accum[v]/(i+1)\n",
    "#                     plt.plot(range(disp_freq+1, i+2, disp_freq), mean_diffs)\n",
    "#                     plt.xlabel(\"Number of samples\")\n",
    "#                     plt.ylabel(\"Mean absolute Fisher difference\")\n",
    "#                     display.display(plt.gcf())\n",
    "#                     display.clear_output(wait=True)\n",
    "\n",
    "#         # divide totals by number of samples\n",
    "#         for v in range(len(self.F_accum)):\n",
    "#             self.F_accum[v] /= num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial loss is:  2.30259\n",
      "The final training loss is:  2.35615\n",
      "The accuracy on the training set is:  0.105\n",
      "The accuracy on the validation set is:  0.0933333333333\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "# we now ask tensorflow to run actual datadata through the graph.\n",
    "# The data must be passed in using the feed_dict argument.\n",
    "\n",
    "# for example, if I want to obtain the initial loss before doing any training:\n",
    "loss = sess.run(loss_f, feed_dict={X: x_tr, y: y_tr})\n",
    "print \"The initial loss is: \", loss\n",
    "\n",
    "# If what you want is to train the network using all the training data, then you have to ask:\n",
    "sess.run(GD_step, feed_dict={X: x_tr, y: y_tr})\n",
    "\n",
    "# you can loop over this to train over more than one epoch.\n",
    "\n",
    "# If you want to obtain the accuracy of the network on the training set:\n",
    "predicted_labels = sess.run(max_y_index, feed_dict={X: x_tr, y: y_tr})\n",
    "tr_acc = np.mean(predicted_labels == np.argmax(y_tr, axis = 1))\n",
    "loss = sess.run(loss_f, feed_dict={X: x_tr, y: y_tr})\n",
    "print \"The final training loss is: \", loss\n",
    "print \"The accuracy on the training set is: \", tr_acc\n",
    "\n",
    "# If you want to obtain the accuracy of the network on the validation set:\n",
    "predicted_labels = sess.run(max_y_index, feed_dict={X: x_vl, y: y_vl})\n",
    "vl_acc = np.mean(predicted_labels == np.argmax(y_vl, axis = 1))\n",
    "print \"The accuracy on the validation set is: \", vl_acc \n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.0995833333333\n",
      "epoch  0  loss  2.30238\n",
      "The accuracy on the validation set is:  0.1\n",
      "The accuracy on the training set is:  0.11\n",
      "epoch  1  loss  2.30217\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.127083333333\n",
      "epoch  2  loss  2.30195\n",
      "The accuracy on the validation set is:  0.1\n",
      "The accuracy on the training set is:  0.154583333333\n",
      "epoch  3  loss  2.30173\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.180833333333\n",
      "epoch  4  loss  2.3015\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.21\n",
      "epoch  5  loss  2.30126\n",
      "The accuracy on the validation set is:  0.1\n",
      "The accuracy on the training set is:  0.2425\n",
      "epoch  6  loss  2.30101\n",
      "The accuracy on the validation set is:  0.1\n",
      "The accuracy on the training set is:  0.266666666667\n",
      "epoch  7  loss  2.30075\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.285\n",
      "epoch  8  loss  2.30049\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.299166666667\n",
      "epoch  9  loss  2.30022\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.315416666667\n",
      "epoch  10  loss  2.29994\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.330416666667\n",
      "epoch  11  loss  2.29966\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.34125\n",
      "epoch  12  loss  2.29937\n",
      "The accuracy on the validation set is:  0.11\n",
      "The accuracy on the training set is:  0.348333333333\n",
      "epoch  13  loss  2.29907\n",
      "The accuracy on the validation set is:  0.11\n",
      "The accuracy on the training set is:  0.360416666667\n",
      "epoch  14  loss  2.29877\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.37125\n",
      "epoch  15  loss  2.29846\n",
      "The accuracy on the validation set is:  0.11\n",
      "The accuracy on the training set is:  0.382083333333\n",
      "epoch  16  loss  2.29814\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.388333333333\n",
      "epoch  17  loss  2.29782\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.398333333333\n",
      "epoch  18  loss  2.2975\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.405833333333\n",
      "epoch  19  loss  2.29718\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.415416666667\n",
      "epoch  20  loss  2.29686\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.42125\n",
      "epoch  21  loss  2.29654\n",
      "The accuracy on the validation set is:  0.123333333333\n",
      "The accuracy on the training set is:  0.43125\n",
      "epoch  22  loss  2.29622\n",
      "The accuracy on the validation set is:  0.123333333333\n",
      "The accuracy on the training set is:  0.43875\n",
      "epoch  23  loss  2.2959\n",
      "The accuracy on the validation set is:  0.123333333333\n",
      "The accuracy on the training set is:  0.440833333333\n",
      "epoch  24  loss  2.29559\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.448333333333\n",
      "epoch  25  loss  2.29527\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.454583333333\n",
      "epoch  26  loss  2.29495\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.462916666667\n",
      "epoch  27  loss  2.29464\n",
      "The accuracy on the validation set is:  0.11\n",
      "The accuracy on the training set is:  0.472083333333\n",
      "epoch  28  loss  2.29432\n",
      "The accuracy on the validation set is:  0.11\n",
      "The accuracy on the training set is:  0.480416666667\n",
      "epoch  29  loss  2.29401\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.4875\n",
      "epoch  30  loss  2.29369\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.494583333333\n",
      "epoch  31  loss  2.29337\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.505416666667\n",
      "epoch  32  loss  2.29306\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.510416666667\n",
      "epoch  33  loss  2.29274\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.517083333333\n",
      "epoch  34  loss  2.29241\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.520833333333\n",
      "epoch  35  loss  2.2921\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.532083333333\n",
      "epoch  36  loss  2.29178\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.539583333333\n",
      "epoch  37  loss  2.29146\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.547083333333\n",
      "epoch  38  loss  2.29114\n",
      "The accuracy on the validation set is:  0.103333333333\n",
      "The accuracy on the training set is:  0.553333333333\n",
      "epoch  39  loss  2.29082\n",
      "The accuracy on the validation set is:  0.1\n",
      "The accuracy on the training set is:  0.565\n",
      "epoch  40  loss  2.2905\n",
      "The accuracy on the validation set is:  0.1\n",
      "The accuracy on the training set is:  0.567916666667\n",
      "epoch  41  loss  2.29017\n",
      "The accuracy on the validation set is:  0.1\n",
      "The accuracy on the training set is:  0.573333333333\n",
      "epoch  42  loss  2.28985\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.58\n",
      "epoch  43  loss  2.28952\n",
      "The accuracy on the validation set is:  0.0933333333333\n",
      "The accuracy on the training set is:  0.580833333333\n",
      "epoch  44  loss  2.28919\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.589583333333\n",
      "epoch  45  loss  2.28886\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.593333333333\n",
      "epoch  46  loss  2.28854\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.599583333333\n",
      "epoch  47  loss  2.28821\n",
      "The accuracy on the validation set is:  0.0966666666667\n",
      "The accuracy on the training set is:  0.605\n",
      "epoch  48  loss  2.28788\n",
      "The accuracy on the validation set is:  0.106666666667\n",
      "The accuracy on the training set is:  0.609583333333\n",
      "epoch  49  loss  2.28755\n",
      "The accuracy on the validation set is:  0.11\n",
      "The accuracy on the training set is:  0.615\n",
      "epoch  50  loss  2.28722\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.619583333333\n",
      "epoch  51  loss  2.28688\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.624166666667\n",
      "epoch  52  loss  2.28655\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.630833333333\n",
      "epoch  53  loss  2.28622\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.634583333333\n",
      "epoch  54  loss  2.28588\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.64\n",
      "epoch  55  loss  2.28555\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.645416666667\n",
      "epoch  56  loss  2.28523\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.648333333333\n",
      "epoch  57  loss  2.2849\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.65\n",
      "epoch  58  loss  2.28457\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.6525\n",
      "epoch  59  loss  2.28426\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.655416666667\n",
      "epoch  60  loss  2.28394\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.655833333333\n",
      "epoch  61  loss  2.28363\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.658333333333\n",
      "epoch  62  loss  2.28331\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.661666666667\n",
      "epoch  63  loss  2.283\n",
      "The accuracy on the validation set is:  0.116666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the training set is:  0.661666666667\n",
      "epoch  64  loss  2.28269\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.664166666667\n",
      "epoch  65  loss  2.28238\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.666666666667\n",
      "epoch  66  loss  2.28206\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.667916666667\n",
      "epoch  67  loss  2.28175\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.67125\n",
      "epoch  68  loss  2.28144\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.674166666667\n",
      "epoch  69  loss  2.28112\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.674583333333\n",
      "epoch  70  loss  2.2808\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.67875\n",
      "epoch  71  loss  2.28048\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.68\n",
      "epoch  72  loss  2.28016\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.6825\n",
      "epoch  73  loss  2.27984\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.685\n",
      "epoch  74  loss  2.27951\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.685416666667\n",
      "epoch  75  loss  2.27919\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.687083333333\n",
      "epoch  76  loss  2.27887\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.69\n",
      "epoch  77  loss  2.27855\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.690416666667\n",
      "epoch  78  loss  2.27823\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.693333333333\n",
      "epoch  79  loss  2.27791\n",
      "The accuracy on the validation set is:  0.113333333333\n",
      "The accuracy on the training set is:  0.69625\n",
      "epoch  80  loss  2.27758\n",
      "The accuracy on the validation set is:  0.116666666667\n",
      "The accuracy on the training set is:  0.698333333333\n",
      "epoch  81  loss  2.27725\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.699166666667\n",
      "epoch  82  loss  2.27693\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.699583333333\n",
      "epoch  83  loss  2.27661\n",
      "The accuracy on the validation set is:  0.123333333333\n",
      "The accuracy on the training set is:  0.702083333333\n",
      "epoch  84  loss  2.27629\n",
      "The accuracy on the validation set is:  0.123333333333\n",
      "The accuracy on the training set is:  0.702916666667\n",
      "epoch  85  loss  2.27598\n",
      "The accuracy on the validation set is:  0.123333333333\n",
      "The accuracy on the training set is:  0.703333333333\n",
      "epoch  86  loss  2.27566\n",
      "The accuracy on the validation set is:  0.123333333333\n",
      "The accuracy on the training set is:  0.705833333333\n",
      "epoch  87  loss  2.27535\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.70875\n",
      "epoch  88  loss  2.27504\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.709583333333\n",
      "epoch  89  loss  2.27472\n",
      "The accuracy on the validation set is:  0.12\n",
      "The accuracy on the training set is:  0.710416666667\n",
      "epoch  90  loss  2.27441\n",
      "The accuracy on the validation set is:  0.126666666667\n",
      "The accuracy on the training set is:  0.711666666667\n",
      "epoch  91  loss  2.27409\n",
      "The accuracy on the validation set is:  0.126666666667\n",
      "The accuracy on the training set is:  0.715416666667\n",
      "epoch  92  loss  2.27378\n",
      "The accuracy on the validation set is:  0.126666666667\n",
      "The accuracy on the training set is:  0.7175\n",
      "epoch  93  loss  2.27346\n",
      "The accuracy on the validation set is:  0.126666666667\n",
      "The accuracy on the training set is:  0.719166666667\n",
      "epoch  94  loss  2.27314\n",
      "The accuracy on the validation set is:  0.126666666667\n",
      "The accuracy on the training set is:  0.722083333333\n",
      "epoch  95  loss  2.27282\n",
      "The accuracy on the validation set is:  0.126666666667\n",
      "The accuracy on the training set is:  0.723333333333\n",
      "epoch  96  loss  2.2725\n",
      "The accuracy on the validation set is:  0.13\n",
      "The accuracy on the training set is:  0.724166666667\n",
      "epoch  97  loss  2.27219\n",
      "The accuracy on the validation set is:  0.133333333333\n",
      "The accuracy on the training set is:  0.7275\n",
      "epoch  98  loss  2.27187\n",
      "The accuracy on the validation set is:  0.133333333333\n",
      "The accuracy on the training set is:  0.729583333333\n",
      "epoch  99  loss  2.27154\n",
      "The accuracy on the validation set is:  0.136666666667\n",
      "The accuracy on the training set is:  0.730416666667\n",
      "epoch  100  loss  2.27122\n",
      "The accuracy on the validation set is:  0.136666666667\n",
      "The accuracy on the training set is:  0.731666666667\n",
      "epoch  101  loss  2.2709\n",
      "The accuracy on the validation set is:  0.136666666667\n",
      "The accuracy on the training set is:  0.732083333333\n",
      "epoch  102  loss  2.27057\n",
      "The accuracy on the validation set is:  0.143333333333\n",
      "The accuracy on the training set is:  0.734583333333\n",
      "epoch  103  loss  2.27024\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.735\n",
      "epoch  104  loss  2.26992\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.737083333333\n",
      "epoch  105  loss  2.26959\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.73875\n",
      "epoch  106  loss  2.26926\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.73875\n",
      "epoch  107  loss  2.26894\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.740833333333\n",
      "epoch  108  loss  2.26862\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.747083333333\n",
      "epoch  109  loss  2.26829\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.75125\n",
      "epoch  110  loss  2.26797\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.750416666667\n",
      "epoch  111  loss  2.26764\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.749583333333\n",
      "epoch  112  loss  2.26732\n",
      "The accuracy on the validation set is:  0.146666666667\n",
      "The accuracy on the training set is:  0.75125\n",
      "epoch  113  loss  2.26699\n",
      "The accuracy on the validation set is:  0.14\n",
      "The accuracy on the training set is:  0.752083333333\n",
      "epoch  114  loss  2.26666\n",
      "The accuracy on the validation set is:  0.14\n",
      "The accuracy on the training set is:  0.751666666667\n",
      "epoch  115  loss  2.26634\n",
      "The accuracy on the validation set is:  0.14\n",
      "The accuracy on the training set is:  0.750833333333\n",
      "epoch  116  loss  2.266\n",
      "The accuracy on the validation set is:  0.14\n",
      "The accuracy on the training set is:  0.7525\n",
      "epoch  117  loss  2.26566\n",
      "The accuracy on the validation set is:  0.143333333333\n",
      "The accuracy on the training set is:  0.753333333333\n",
      "epoch  118  loss  2.26532\n",
      "The accuracy on the validation set is:  0.143333333333\n",
      "The accuracy on the training set is:  0.752916666667\n",
      "epoch  119  loss  2.26499\n",
      "The accuracy on the validation set is:  0.143333333333\n",
      "The accuracy on the training set is:  0.754166666667\n",
      "The accuracy on the validation set is:  0.143333333333\n"
     ]
    }
   ],
   "source": [
    "print \"Starting\"\n",
    "nepochs = 120\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "test()\n",
    "# losses = train(sess, nepochs=10)\n",
    "# print losses\n",
    "losses = np.ndarray(shape=(nepochs), dtype=float)\n",
    "for i in range(nepochs):\n",
    "#         x, y = tf.train.batch([x_tr, y_tr], batch_size)\n",
    "#     inds = range(N)\n",
    "#     np.random.shuffle(inds)\n",
    "#     x_tr, y_tr = x_tr(inds), y_tr(inds)\n",
    "    sess.run(GD_step, feed_dict={X: x_tr, y: y_tr})\n",
    "    loss = sess.run(loss_f, feed_dict={X: x_tr, y: y_tr})\n",
    "    losses[i] = loss\n",
    "    print \"epoch \", i, \" loss \", loss\n",
    "    test()\n",
    "test()\n",
    "sess.close()\n",
    "graph(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print x_tr\n",
    "# x, y = get_batch(2)\n",
    "# print type(x_tr)\n",
    "# print type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(x_vl)\n",
    "# print type(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
