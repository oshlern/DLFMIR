{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network with Tensorflow #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built a tensorflow from scratch in Julia. Going through such process is crucial for us to be able to understand how Neural Networks learn and the mathematics that make it possible. \n",
    "\n",
    "Several tools available today make the building of Neural Networks an easy task, where the math becomes second nature, and the emergent properties of the Neural Network, such as its architecture, become the main focus.\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/) is such a tool. This notebook will guide you through the steps necessary to build, with Tenforflow, a Feedforward Neural Network identical to the last one we built in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's reload the music dataset we used previously\n",
    "Iowa_MIS_dataset = scipy.io.loadmat('Iowa_MIS_dataset.mat')\n",
    "data = Iowa_MIS_dataset['dat_all']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: the variable `data` is of size [660,88201], and is is already shuffled (it's always good to double check, just to make sure). The number of rows tells you the number of datapoints in the dataset, and the number of columns - 1 tells you the dimensionality of the dataset. The rightmost column in the matrix contains the labels for all datapoints. Possible labels are:\n",
    "\n",
    "0 'Bass'\n",
    "\n",
    "1 'Bassoon'\n",
    "\n",
    "2 'Cello'\n",
    "\n",
    "3 'Clarinet'\n",
    "\n",
    "4 'Flute'\n",
    "\n",
    "5 'Guitar'\n",
    "\n",
    "6 'Horn'\n",
    "\n",
    "7 'Sax'\n",
    "\n",
    "8 'Trombone'\n",
    "\n",
    "9 'Trumpet'\n",
    "\n",
    "10 'Viola'\n",
    "\n",
    "11 'Violin'\n",
    "\n",
    "Now separate the datapoints into training (~80% of the data), validation (~10%), and test sets (~10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general data parameters\n",
    "N = data.shape[0]\n",
    "D = data.shape[1]-1\n",
    "C = 12\n",
    "\n",
    "# split into training, validation, and test sets\n",
    "perc_tr = 0.8\n",
    "perc_vl = 0.1\n",
    "perc_ts = 0.1\n",
    "\n",
    "# randomly shuffle the data (just to make sure)\n",
    "np.random.permutation(data)\n",
    "\n",
    "# separate into training, validation, and test sets\n",
    "# data\n",
    "x_tr = data[:int(N*perc_tr),0:-1]\n",
    "x_vl = data[int(N*perc_tr):int(N*perc_tr+N*perc_vl),0:-1]\n",
    "x_ts = data[-int(N*perc_ts):,0:-1]\n",
    "# labels\n",
    "y_tr = data[:int(N*perc_tr),-1].reshape(int(N*perc_tr),1)\n",
    "y_vl = data[int(N*perc_tr):int(N*perc_tr+N*perc_vl),-1].reshape(int(N*perc_vl),1)\n",
    "y_ts = data[-int(N*perc_ts):,-1].reshape(int(N*perc_ts),1)\n",
    "\n",
    "# we won't use the variable `data` after this point.\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow will expect the data labels to be one-hot encodings with a $1$ at the index corresponding to teh correct label. The following snipper should fix this four you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = np.zeros((y_tr.shape[0],C))\n",
    "temp[np.arange(y_tr.shape[0]),y_tr.astype(int)[:,0]] = 1\n",
    "y_tr = temp\n",
    "temp = np.zeros((y_vl.shape[0],C))\n",
    "temp[np.arange(y_vl.shape[0]),y_vl.astype(int)[:,0]] = 1\n",
    "y_vl = temp\n",
    "temp = np.zeros((y_ts.shape[0],C))\n",
    "temp[np.arange(y_ts.shape[0]),y_ts.astype(int)[:,0]] = 1\n",
    "y_ts = temp\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded as numpy arrays, we will build the feedforward Neural Network.\n",
    "\n",
    "Remember that the expression describing the forward pass is:\n",
    "\n",
    "$$\\hat{y} = softmax(~\\sigma(x~W^{(h)})~W^{(o)})$$\n",
    "\n",
    "And that the training objective is:\n",
    "\n",
    "$$J = minimize\\{\\hat{y}_{correct}\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining all the general parameters that we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general parameters\n",
    "N = x_tr.shape[0] # number of training examples\n",
    "D = x_tr.shape[1] # dimensionality of the data\n",
    "C = y_tr.shape[1] # number of unique labels in the dataset\n",
    "\n",
    "# hyperparameters\n",
    "H = 128 # number of hidden units. In general try to stick to a power of 2\n",
    "lr = 0.01 # the learning rate (previously refered to in the notes as alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will have to initialize the tensorflow \"variables\", and the tensorflow \"placeholders\".\n",
    "\n",
    "The \"variables\" are the ones that tensorflow will be able to learn, given a neural network architecture and an objective function. \n",
    "\n",
    "Will out training data be a \"variable\" or a \"placeholder\"? What about the weights?\n",
    "\n",
    "After that we will build and train the model also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial loss is:  2.48516\n",
      "The final training loss is:  2.48272\n",
      "The accuracy on the training set is:  0.123106060606\n",
      "The accuracy on the validation set is:  0.0757575757576\n"
     ]
    }
   ],
   "source": [
    "# let's initialize the weights\n",
    "W_h = tf.Variable(tf.random_normal((D,H), stddev = 0.01)) # mean=0.0\n",
    "W_o = tf.Variable(tf.random_normal((H,C), stddev = 0.01)) # mean=0.0\n",
    "\n",
    "# We now initialize the placeholders for our data. At this point we won't be passing any data in yet\n",
    "# Tensorflow build a graph with the information that we are giving it, so it just allocates graph space\n",
    "# for the data that we will pass in\n",
    "# we will have to tell it the datatype (\"float\"), and the shape of the data\n",
    "X = tf.placeholder(\"float\", shape=[None,D])\n",
    "y = tf.placeholder(\"float\", shape=[None,C])\n",
    "\n",
    "# we now do the forward pass until we obtain the scores\n",
    "h = tf.nn.sigmoid(tf.matmul(X,W_h))\n",
    "scores = tf.matmul(h, W_o)\n",
    "\n",
    "# now that we have the scores, we can turn that into probabilities\n",
    "y_hat = tf.nn.softmax(scores)\n",
    "# and see which of the class indices received the largest value\n",
    "max_y_index = tf.argmax(y_hat, axis = 1)\n",
    "\n",
    "# now we have to do the backpropagation, which in tensorflow will be extremely easy.\n",
    "# 1st we compute the cross entropy loss using tensorflow's softmax_cross_entropy_with_logits function\n",
    "loss_f = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=scores))\n",
    "# 2nd we ask tensorflow to compute the gradients with respect to all the tensorflow \"variables\"\n",
    "# which for us are the trainable weights. We will have to tell it what the objective is, and it will\n",
    "# also need a \"rate\" (lr) with which to proceed toward that objetive using Gradient Descent.\n",
    "GD_step = tf.train.GradientDescentOptimizer(lr).minimize(loss_f)\n",
    "\n",
    "# everything we have done so far has set up the tensorflow graph, but will not make the neural\n",
    "# network learn. For learning to take place, we need to initialize a tensorflow session and initialize \n",
    "# all the variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# we now ask tensorflow to run actual datadata through the graph.\n",
    "# The data must be passed in using the feed_dict argument.\n",
    "\n",
    "# for example, if I want to obtain the initial loss before doing any training:\n",
    "loss = sess.run(loss_f, feed_dict={X: x_tr, y: y_tr})\n",
    "print \"The initial loss is: \", loss\n",
    "\n",
    "# If what you want is to train the network using all the training data, then you have to ask:\n",
    "sess.run(GD_step, feed_dict={X: x_tr, y: y_tr})\n",
    "\n",
    "# you can loop over this to train over more than one epoch.\n",
    "nepochs = 15\n",
    "for i in range(nepochs):\n",
    "    sess.run(GD_step, feed_dict={X: x_tr, y: y_tr})\n",
    "\n",
    "# If you want to obtain the accuracy of the network on the training set:\n",
    "predicted_labels = sess.run(max_y_index, feed_dict={X: x_tr, y: y_tr})\n",
    "tr_acc = np.mean(predicted_labels == np.argmax(y_tr, axis = 1))\n",
    "loss = sess.run(loss_f, feed_dict={X: x_tr, y: y_tr})\n",
    "print \"The final training loss is: \", loss\n",
    "print \"The accuracy on the training set is: \", tr_acc\n",
    "\n",
    "# If you want to obtain the accuracy of the network on the validation set:\n",
    "predicted_labels = sess.run(max_y_index, feed_dict={X: x_vl, y: y_vl})\n",
    "vl_acc = np.mean(predicted_labels == np.argmax(y_vl, axis = 1))\n",
    "print \"The accuracy on the validation set is: \", vl_acc\n",
    "                 \n",
    "sess.close()                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this network a little better in the following ways:\n",
    "\n",
    "1. Updating the forward pass so that now we have \"biases\" in addition to the weights (why would this be helpful?)\n",
    "\n",
    "$$\\hat{y} = softmax(~\\sigma(x~W^{(h)} + b^{(h)})~W^{(o)} + b^{(o)})$$\n",
    "\n",
    "2. Showing the network not all data at once, but iterating over randomly-generated mini-batches.\n",
    "\n",
    "3. Can you think of other ways?\n",
    "\n",
    "Implement at least the first two points above, and any other ideas you can think about to improve this NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
